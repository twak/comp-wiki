<html>
   <head> <meta name="viewport" content="width=device-width, initial-scale=1.0"> </head>
<body>
 <p><a href="../index.html">wiki home</a><p>

<p>
<b> compute at leeds</b>. note that Leeds' uses <i>cluster</i> to mean "lab" and <i>research compute</i> when they mean "cluster". (I mean technically, I guess we have clusters of urninals in many gent's bathrooms, I still wouldn't write cluster on the door)
</p>
      
<p>
Main compute resources are the local ARC, regional <https://docs.hpc.shef.ac.uk/en/latest/bede.html#>>Bede</a> (why Power9 <a href="https://www.n8research.org.uk/">N8</a>, why - what did IBM do to achieve this?!), and national <a href="https://docs.hpc.shef.ac.uk/en/latest/jade2.html">Jade 2</a> (not yet online as of March 2021).
</p>

<p>
   Resources for <emph>using ARC</emph> are <a href="https://arcdocs.leeds.ac.uk/welcome.html">here</a> + <a href="./arc_cheat_sheet.pdf">Rebecca's cheat sheet</a>.
   </p>
   
<p>
Comparing different clusters is non-trivial. This table just counts the number of GPUs - this is normally what ML-ey graphics-ey people use. We note that server-side GPU is <a href="https://www.theregister.com/2018/01/03/nvidia_server_gpus/">expensive</a> pared to (gamer grade) local (under desk GPUs). Local is also much easier to debug (trying to debug something run with a <a href="https://gpuhackshef.readthedocs.io/en/latest/bede/scheduler.html">scheduler</a> is a nightmare). (a list of <a href="https://www.hpc-uk.ac.uk/facilities/">national HPC facilities</a>).
</p>
   
<p>
<table  border="1">
<tr><td><b>institution</b></td><td><b>cluster</b></td><td><b>GPU hardware</b></td><td><b>GPU count</b></td></tr>
   
<tr><td>York</td><td><a href="https://www.york.ac.uk/it-services/services/viking-computing-cluster/">Viking</a></td><td>8 x v100</td><td>8</td></tr>
<tr><td>Ireland/ICHEC</td><td><a href="https://www.ichec.ie/about/infrastructure/kay/">Kay</a></td><td>32 x v100</td><td>32</td></tr>
<tr><td><b>Leeds</b></td><td><a href="https://arcdocs.leeds.ac.uk/systems/start.html">ARC 3/4</a></td><td>4 x k80, 24 x P100, 12 x V100</td><td>40</td></tr>
<tr><td><b>Bede (Tier 2)</b></td><td><a href="https://docs.hpc.shef.ac.uk/en/latest/bede.html#">NICE-19</a></td><td>80 x V100 (IBM Power9)</td><td>80</td></tr>
<tr><td>Cirrus (Tier 2)</td><td><a href="https://cirrus.readthedocs.io/en/master/user-guide/gpu.html">Cirrus</a></td><td>152 x v100</td><td>152</td></tr>
<tr><td>Bristol</td><td><a href="https://www.bristol.ac.uk/acrc/high-performance-computing/hpc-systems-tech-specs/">BlueCrystal 3/4</a></td><td>76 x k20, 65 x p100</td><td>141</td></tr>
<tr><td>Jade</td><td><a href="https://www.jade.ac.uk/">v1</a></td><td>176 x V100</td><td>176</td></tr>
<tr><td>Edinburgh</td><td><a href= "https://www.ed.ac.uk/information-services/research-support/research-computing/ecdf/high-performance-computing">Eddie</a> + <a href="http://computing.help.inf.ed.ac.uk/james-and-charles-cluster">James and Charles</a></td><td>eddie ( 44 x k80, 80 x titan X) + james (59 mixed) </td><td>183</td></tr>
<tr><td>Birmingham</td><td><a href= "https://www.baskerville.ac.uk/">Baskerville</a> (July 21)</td><td>184 x a100</td><td>184</td></tr>
<tr><td>Cambridge (Tier 2)</td><td><a href="https://www.hpc.cam.ac.uk/systems/wilkes-2">Wilkes2</a></td><td>360 x p100</td><td>360</td></tr>
<tr><td>KAUST</td><td><a href="https://www.hpc.kaust.edu.sa/ibex/gpu_nodes">Ibext</a></td><td>32 x 2080ti, 32 x 1080ti, 20 x P100, 4 x p6000, 272xv100</td><td>340</td></tr>
<tr><td>Jade 2 (Tier 2)</td><td><a href="https://docs.hpc.shef.ac.uk/en/latest/jade2.html">v2</a></td><td>504 x V100</td><td>504</td></tr>
  <tr><td>UMass</td><td><a href="https://mghpcc.umass.edu//">MGHPCC</a></td><td>unspecified mix</td><td>1092</td></tr>

</table>
</p>

<body>
</html>
